{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import os \n",
    "from train import *\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "from early_exit_resnet import EarlyExitResNet50\n",
    "from calibrate import *\n",
    "from torch.utils.data import DataLoader, Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example instantiation of the model\n",
    "model = EarlyExitResNet50(num_classes=10)\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "# Move the model to the specified device\n",
    "model.load_state_dict(torch.load(\"models/resnet50.pth\"))\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),\"models/resnet50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "# Load the training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load the original test dataset\n",
    "full_testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = DataLoader(full_testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# # Define the size of the validation set\n",
    "# valid_size = int(0.5 * len(full_testset))  # for example, 50% of the test set\n",
    "\n",
    "# # Generate indices: here, we shuffle the indices of the full test set and then split\n",
    "# indices = torch.randperm(len(full_testset)).tolist()\n",
    "# valid_indices, test_indices = indices[:valid_size], indices[valid_size:]\n",
    "\n",
    "# # Create validation and test subsets\n",
    "# validset = Subset(full_testset, valid_indices)\n",
    "# testset = Subset(full_testset, test_indices)\n",
    "\n",
    "# # Create DataLoader instances for the validation and test sets\n",
    "# validloader = DataLoader(validset, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for exit 1: 72.17%\n",
      "Accuracy for exit 2: 74.10%\n",
      "Accuracy for exit 3: 77.60%\n",
      "Accuracy for exit 4: 78.07%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracies for each exit\n",
    "accuracies = evaluate_model(model, testloader, device)\n",
    "for i, acc in enumerate(accuracies, 1):\n",
    "    print(f'Accuracy for exit {i}: {acc * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training your model\n",
    "opt_temp_exit1, opt_temp_exit2, opt_temp_final = calibrate_model_exits(model, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"resnet50-calibrated.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_temperature_scaling(model, inputs, temp_exit1, temp_exit2, temp_final):\n",
    "    model.eval()\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        output_early_exit1, output_early_exit2, output_final = model(inputs)\n",
    "        \n",
    "        # Apply temperature scaling to each exit's logits\n",
    "        scaled_output_early_exit1 = output_early_exit1 / temp_exit1\n",
    "        scaled_output_early_exit2 = output_early_exit2 / temp_exit2\n",
    "        scaled_output_final = output_final / temp_final\n",
    "        \n",
    "        # Calculate softmax probabilities for each exit\n",
    "        probs_early_exit1 = softmax(scaled_output_early_exit1).cpu().numpy()\n",
    "        probs_early_exit2 = softmax(scaled_output_early_exit2).cpu().numpy()\n",
    "        probs_final = softmax(scaled_output_final).cpu().numpy()\n",
    "\n",
    "    return probs_early_exit1, probs_early_exit2, probs_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inference_with_temperature_scaling() takes 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate softmax probabilities with temperature scaling for each exit\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m probs_early_exit1, probs_early_exit2, probs_final \u001b[38;5;241m=\u001b[39m \u001b[43minference_with_temperature_scaling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_temp_exit1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_temp_exit2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_temp_final\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Append probabilities and true labels for later analysis\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Assuming probs_list_early_exit1, probs_list_early_exit2, probs_list_final, and true_labels_list are defined earlier\u001b[39;00m\n\u001b[0;32m     23\u001b[0m probs_list_early_exit1\u001b[38;5;241m.\u001b[39mappend(probs_early_exit1)\n",
      "\u001b[1;31mTypeError\u001b[0m: inference_with_temperature_scaling() takes 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Assuming testloader, model, and device are defined\n",
    "# Initialize lists to store probabilities and true labels\n",
    "probs_list_early_exit1, probs_list_early_exit2, probs_list_final = [], [], []\n",
    "true_labels_list = []\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Calculate softmax probabilities with temperature scaling for each exit\n",
    "        probs_early_exit1, probs_early_exit2, probs_final = inference_with_temperature_scaling(\n",
    "            model, inputs, opt_temp_exit1, opt_temp_exit2, opt_temp_final\n",
    "        )\n",
    "        \n",
    "        # Append probabilities and true labels for later analysis\n",
    "        # Assuming probs_list_early_exit1, probs_list_early_exit2, probs_list_final, and true_labels_list are defined earlier\n",
    "        probs_list_early_exit1.append(probs_early_exit1)\n",
    "        probs_list_early_exit2.append(probs_early_exit2)\n",
    "        probs_list_final.append(probs_final)\n",
    "        true_labels_list.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate all collected arrays\n",
    "probs_early_exit1 = np.concatenate(probs_list_early_exit1, axis=0)\n",
    "probs_early_exit2 = np.concatenate(probs_list_early_exit2, axis=0)\n",
    "probs_final = np.concatenate(probs_list_final, axis=0)\n",
    "true_labels = np.concatenate(true_labels_list, axis=0)\n",
    "\n",
    "# Function to plot reliability diagram\n",
    "def plot_reliability_diagram(y_true, y_prob, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    for i in range(y_prob.shape[1]):  # Assuming y_prob.shape[1] is the number of classes\n",
    "        true_class = (y_true == i)\n",
    "        prob_true_class = y_prob[:, i]\n",
    "        \n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(true_class, prob_true_class, n_bins=10)\n",
    "        \n",
    "        ax.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=f\"Class {i}\")\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    ax.set_ylabel(\"Fraction of positives\")\n",
    "    ax.set_xlabel(\"Mean predicted value\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "# Plotting reliability diagrams for each exit\n",
    "plot_reliability_diagram(true_labels, probs_early_exit1, \"Reliability Diagram for Early Exit 1\")\n",
    "plot_reliability_diagram(true_labels, probs_early_exit2, \"Reliability Diagram for Early Exit 2\")\n",
    "plot_reliability_diagram(true_labels, probs_final, \"Reliability Diagram for Final Exit\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inference_with_temperature_scaling() takes 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m uncal_probs_early_exit1, uncal_probs_early_exit2, uncal_probs_final \u001b[38;5;241m=\u001b[39m inference_without_temperature_scaling(model, inputs)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Generate calibrated probabilities\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m cal_probs_early_exit1, cal_probs_early_exit2, cal_probs_final \u001b[38;5;241m=\u001b[39m \u001b[43minference_with_temperature_scaling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_temp_exit1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_temp_exit2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_temp_final\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Append uncalibrated probabilities and true labels for later analysis\u001b[39;00m\n\u001b[0;32m     34\u001b[0m uncalibrated_probs_list_early_exit1\u001b[38;5;241m.\u001b[39mappend(uncal_probs_early_exit1)\n",
      "\u001b[1;31mTypeError\u001b[0m: inference_with_temperature_scaling() takes 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Function to generate uncalibrated softmax probabilities\n",
    "def inference_without_temperature_scaling(model, inputs):\n",
    "    model.eval()\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        output_early_exit1, output_early_exit2, output_final = model(inputs)\n",
    "        \n",
    "        probs_early_exit1 = softmax(output_early_exit1).cpu().numpy()\n",
    "        probs_early_exit2 = softmax(output_early_exit2).cpu().numpy()\n",
    "        probs_final = softmax(output_final).cpu().numpy()\n",
    "\n",
    "    return probs_early_exit1, probs_early_exit2, probs_final\n",
    "\n",
    "# Assuming testloader, model, opt_temp_exit1, opt_temp_exit2, opt_temp_final, and device are defined\n",
    "# Initialize lists to store probabilities and true labels\n",
    "uncalibrated_probs_list_early_exit1, uncalibrated_probs_list_early_exit2, uncalibrated_probs_list_final = [], [], []\n",
    "calibrated_probs_list_early_exit1, calibrated_probs_list_early_exit2, calibrated_probs_list_final = [], [], []\n",
    "true_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Generate uncalibrated probabilities\n",
    "        uncal_probs_early_exit1, uncal_probs_early_exit2, uncal_probs_final = inference_without_temperature_scaling(model, inputs)\n",
    "        \n",
    "        # Generate calibrated probabilities\n",
    "        cal_probs_early_exit1, cal_probs_early_exit2, cal_probs_final = inference_with_temperature_scaling(model, inputs, opt_temp_exit1, opt_temp_exit2, opt_temp_final)\n",
    "        \n",
    "        # Append uncalibrated probabilities and true labels for later analysis\n",
    "        uncalibrated_probs_list_early_exit1.append(uncal_probs_early_exit1)\n",
    "        uncalibrated_probs_list_early_exit2.append(uncal_probs_early_exit2)\n",
    "        uncalibrated_probs_list_final.append(uncal_probs_final)\n",
    "        \n",
    "        # Append calibrated probabilities for later analysis\n",
    "        calibrated_probs_list_early_exit1.append(cal_probs_early_exit1)\n",
    "        calibrated_probs_list_early_exit2.append(cal_probs_early_exit2)\n",
    "        calibrated_probs_list_final.append(cal_probs_final)\n",
    "        \n",
    "        # Append true labels\n",
    "        true_labels_list.append(labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncalibrated_probs_early_exit1 = np.concatenate(uncalibrated_probs_list_early_exit1, axis=0)\n",
    "uncalibrated_probs_early_exit2 = np.concatenate(uncalibrated_probs_list_early_exit2, axis=0)\n",
    "uncalibrated_probs_final = np.concatenate(uncalibrated_probs_list_final, axis=0)\n",
    "\n",
    "calibrated_probs_early_exit1 = np.concatenate(calibrated_probs_list_early_exit1, axis=0)\n",
    "calibrated_probs_early_exit2 = np.concatenate(calibrated_probs_list_early_exit2, axis=0)\n",
    "calibrated_probs_final = np.concatenate(calibrated_probs_list_final, axis=0)\n",
    "\n",
    "true_labels = np.concatenate(true_labels_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def plot_reliability_diagram(y_true, y_prob_uncalibrated, y_prob_calibrated, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    color_sequence = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    \n",
    "    for i in range(y_prob_uncalibrated.shape[1]):  # Assuming y_prob.shape[1] is the number of classes\n",
    "        true_class = (y_true == i)\n",
    "        prob_true_class_uncalibrated = y_prob_uncalibrated[:, i]\n",
    "        prob_true_class_calibrated = y_prob_calibrated[:, i]\n",
    "        \n",
    "        fraction_of_positives_uncal, mean_predicted_value_uncal = calibration_curve(true_class, prob_true_class_uncalibrated, n_bins=10)\n",
    "        fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(true_class, prob_true_class_calibrated, n_bins=10)\n",
    "        \n",
    "        ax.plot(mean_predicted_value_uncal, fraction_of_positives_uncal, \"s-\", label=f\"Uncalibrated Class {i}\", color=color_sequence[i % len(color_sequence)])\n",
    "        ax.plot(mean_predicted_value_cal, fraction_of_positives_cal, \"^-\", label=f\"Calibrated Class {i}\", color=color_sequence[i % len(color_sequence)])\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    ax.set_ylabel(\"Fraction of positives\")\n",
    "    ax.set_xlabel(\"Mean predicted value\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting reliability diagrams for each exit\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plot_reliability_diagram(\u001b[43mtrue_labels\u001b[49m, uncalibrated_probs_early_exit1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReliability Diagram for Early Exit 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# plot_reliability_diagram(true_labels, uncalibrated_probs_early_exit2, calibrated_probs_early_exit2, \"Reliability Diagram for Early Exit 2\")\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# plot_reliability_diagram(true_labels, uncalibrated_probs_final, calibrated_probs_final, \"Reliability Diagram for Final Exit\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting reliability diagrams for each exit\n",
    "plot_reliability_diagram(true_labels, uncalibrated_probs_early_exit1, \"Reliability Diagram for Early Exit 1\")\n",
    "# plot_reliability_diagram(true_labels, uncalibrated_probs_early_exit2, calibrated_probs_early_exit2, \"Reliability Diagram for Early Exit 2\")\n",
    "# plot_reliability_diagram(true_labels, uncalibrated_probs_final, calibrated_probs_final, \"Reliability Diagram for Final Exit\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "def plot_reliability_diagram_with_cumulative_counts(y_true, y_prob, title):\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "    ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color_sequence = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    n_bins = 10\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    \n",
    "    total_counts = np.zeros(n_bins)\n",
    "    \n",
    "    for i in range(y_prob.shape[1]):  # Assuming y_prob.shape[1] is the number of classes\n",
    "        true_class = (y_true == i)\n",
    "        prob_true_class = y_prob[:, i]\n",
    "\n",
    "        # Calibration curve\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(true_class, prob_true_class, n_bins=n_bins, strategy='uniform')\n",
    "        \n",
    "        # Plot calibration curve\n",
    "        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=f\"Class {i}\", color=color_sequence[i % len(color_sequence)])\n",
    "\n",
    "        # Compute histogram for the predicted probabilities\n",
    "        counts, _ = np.histogram(prob_true_class, bins=bins)\n",
    "        total_counts += counts\n",
    "        \n",
    "    # Cumulative bar plot\n",
    "    ax2.bar(bins[:-1], total_counts, width=bins[1]-bins[0], align='edge', color='gray', alpha=0.3, label='Counts')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_xlabel(\"Mean predicted value\")\n",
    "    ax1.set_title(title)\n",
    "    ax1.legend(loc=\"best\")\n",
    "\n",
    "    ax2.set_ylabel(\"Counts\", color='gray')\n",
    "    ax2.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room for the second y-axis\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_reliability_diagram_with_cumulative_counts() missing 3 required positional arguments: 'y_true', 'y_prob', and 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_reliability_diagram_with_cumulative_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_reliability_diagram_with_cumulative_counts() missing 3 required positional arguments: 'y_true', 'y_prob', and 'title'"
     ]
    }
   ],
   "source": [
    "plot_reliability_diagram_with_cumulative_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
